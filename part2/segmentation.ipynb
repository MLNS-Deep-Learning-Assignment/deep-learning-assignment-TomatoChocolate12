{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of the images into separate digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, npy_files, transform=None):\n",
    "        self.npy_files = npy_files\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = []\n",
    "        for npy_file in self.npy_files:\n",
    "            batch = np.load(npy_file)  # Shape: (10000, 40, 168)\n",
    "            self.data.extend(batch)\n",
    "        self.data = np.array(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]  # Shape: (40, 168)\n",
    "        \n",
    "        # Convert grayscale to RGB by repeating along the channel dimension\n",
    "        image = np.stack([image] * 3, axis=-1)  # Shape: (40, 168, 3)\n",
    "        image = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "        # Generate dummy bounding boxes and masks\n",
    "        # For now, bounding boxes and masks are placeholders.\n",
    "        height, width = image.size\n",
    "        boxes = torch.tensor([[0, 0, width // 2, height // 2]], dtype=torch.float32)  # Placeholder box\n",
    "        labels = torch.tensor([1], dtype=torch.int64)  # Single label\n",
    "        masks = torch.zeros((1, height, width), dtype=torch.uint8)  # Placeholder mask\n",
    "\n",
    "        # Apply transforms if available\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "\n",
    "\n",
    "def visualize_predictions(image, predictions):\n",
    "    \"\"\"Visualize the original image with predicted masks overlaid.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    for box, label, score in zip(predictions[\"boxes\"], predictions[\"labels\"], predictions[\"scores\"]):\n",
    "        if score > 0.5:  # Display high-confidence predictions\n",
    "            x1, y1, x2, y2 = box\n",
    "            plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                              fill=False, edgecolor='red', linewidth=2))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_cropped_digits(image, predictions, output_dir):\n",
    "    \"\"\"Save cropped images of each digit.\"\"\"\n",
    "    for i, box in enumerate(predictions[\"boxes\"]):\n",
    "        x1, y1, x2, y2 = box.int().tolist()\n",
    "        cropped = image[:, y1:y2, x1:x2]\n",
    "        save_path = os.path.join(output_dir, f\"digit_{i}.png\")\n",
    "        torchvision.utils.save_image(cropped, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000, 40, 168)\n",
      "Data type: uint8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file_path = '../data/data0.npy'  # Replace with an actual file path\n",
    "data = np.load(file_path)\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Data type:\", data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "# Paths to all .npy files\n",
    "npy_files = sorted(glob.glob('../data/data*.npy'))\n",
    "\n",
    "# Transformations\n",
    "transforms = Compose([\n",
    "    Resize((128, 128)),  # Resize images to a consistent size\n",
    "    ToTensor(),          # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = DigitSegmentationDataset(npy_files, transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "# Test DataLoader\n",
    "# for images, indices in dataloader:\n",
    "#     print(\"Batch of images:\", images.shape)  # Example: (8, 3, 128, 128)\n",
    "#     print(\"Indices:\", indices)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/yashas.b/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home2/yashas.b/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Mask R-CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Update the classifier for the number of classes (2: background and digits)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)  # 2 classes\n",
    "\n",
    "# Update the mask predictor to match output channels of the feature extractor\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "    in_channels=256,  # Number of channels from the feature extractor (256 after FPN)\n",
    "    num_classes=2,     # Number of classes (background + digits)\n",
    "    dim_reduced=2\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Hyperparameters\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  39%|███▉      | 1476/3750 [15:50<24:23,  1.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Save the Trained Model\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to the device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(model.state_dict(), \"mask_rcnn_digit_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_images = next(iter(dataloader))[0]  # Get a batch of test images\n",
    "test_images = [img.to(device) for img in test_images]\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_images)\n",
    "\n",
    "# Visualize Predictions\n",
    "for idx, pred in enumerate(predictions):\n",
    "    print(f\"Image {idx+1}:\")\n",
    "    boxes = pred[\"boxes\"].cpu().numpy()\n",
    "    scores = pred[\"scores\"].cpu().numpy()\n",
    "    masks = pred[\"masks\"].cpu().numpy()\n",
    "\n",
    "    # Filter by a confidence threshold (e.g., 0.5)\n",
    "    high_conf_indices = scores > 0.5\n",
    "    boxes = boxes[high_conf_indices]\n",
    "    masks = masks[high_conf_indices]\n",
    "\n",
    "    print(f\"  Detected {len(boxes)} objects\")\n",
    "    for i, box in enumerate(boxes):\n",
    "        print(f\"    Box {i+1}: {box}, Score: {scores[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
