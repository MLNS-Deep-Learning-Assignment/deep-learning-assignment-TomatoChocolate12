{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is loaded here and an inference is made by using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from baselineCNN import DigitSumDataset, CNNBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model_path, test_data_pattern='test_data*.npy', test_label_pattern='test_lab*.npy'):\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = CNNBaseline().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Print training metrics from checkpoint\n",
    "    print(\"\\nModel checkpoint info:\")\n",
    "    print(f\"Saved at epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Training loss: {checkpoint['train_loss']:.4f}\")\n",
    "    print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    if 'metrics' in checkpoint:\n",
    "        print(\"Best validation metrics:\")\n",
    "        for k, v in checkpoint['metrics'].items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Data transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    # Load test dataset\n",
    "    test_dataset = DigitSumDataset(\n",
    "        data_files_pattern=test_data_pattern,\n",
    "        label_files_pattern=test_label_pattern,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating test set...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to tensors for easier computation\n",
    "    predictions = torch.tensor(all_predictions)\n",
    "    true_labels = torch.tensor(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    floor_pred = torch.floor(predictions.squeeze())\n",
    "    ceil_pred = torch.ceil(predictions.squeeze())\n",
    "    round_pred = torch.round(predictions.squeeze())\n",
    "\n",
    "    floor_accuracy = (floor_pred == true_labels).float().mean().item()\n",
    "    ceil_accuracy = (ceil_pred == true_labels).float().mean().item()\n",
    "    round_accuracy = (round_pred == true_labels).float().mean().item()\n",
    "    mae = torch.abs(predictions.squeeze() - true_labels).mean().item()\n",
    "    mse = ((predictions.squeeze() - true_labels) ** 2).mean().item()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTest Set Results:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Floor Accuracy: {floor_accuracy:.4f}\")\n",
    "    print(f\"Ceiling Accuracy: {ceil_accuracy:.4f}\")\n",
    "    print(f\"Rounded Accuracy: {round_accuracy:.4f}\")\n",
    "\n",
    "    # Plot prediction distribution\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Prediction vs True Value scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(true_labels.numpy(), predictions.squeeze().numpy(), alpha=0.5)\n",
    "    plt.plot([min(true_labels), max(true_labels)], [min(true_labels), max(true_labels)], 'r--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Predictions vs True Values')\n",
    "\n",
    "    # Error distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    errors = predictions.squeeze().numpy() - true_labels.numpy()\n",
    "    plt.hist(errors, bins=50)\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Error Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_results.png')\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'floor_accuracy': floor_accuracy,\n",
    "        'ceil_accuracy': ceil_accuracy,\n",
    "        'round_accuracy': round_accuracy,\n",
    "        'predictions': predictions.numpy(),\n",
    "        'true_labels': true_labels.numpy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " results = evaluate_test_set(\n",
    "        model_path='best_digit_sum_model.pth',\n",
    "        test_data_pattern='test_data*.npy',\n",
    "        test_label_pattern='test_lab*.npy'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
